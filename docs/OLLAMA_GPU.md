# Ollama: использование GPU на удалённом сервере

Наш проект только отправляет HTTP-запросы к Ollama (chat, list models). **Решение о том, использовать GPU или CPU, принимает только сам Ollama на сервере.** В коде мы не передаём параметров, принудительно включающих или отключающих GPU.

Если на удалённом сервере (например, с RTX 3090) при запросах нагружается только CPU, а видеокарта простаивает — проблема на стороне **сервера Ollama**, а не нашего приложения.

---

## 1. Быстрая проверка с нашей стороны

Скрипт `scripts/check_ollama_gpu.py` опрашивает ваш Ollama host (из конфига) и выводит:

- список загруженных моделей и **использование VRAM** (`size_vram` из `/api/ps`);
- если у загруженной модели `size_vram > 0` — Ollama использует GPU под эту модель;
- если моделей нет в памяти или `size_vram == 0` — подсказки по проверке на сервере.

Запуск (из корня проекта, с тем же конфигом, что и приложение):

```bash
python3 scripts/check_ollama_gpu.py
```

---

## 2. Проверка на самом сервере Ollama

Выполнять нужно **на машине, где запущен Ollama** (у вас — удалённый сервер с RTX 3090).

### 2.1 Нагрузка на GPU

- **VRAM (Memory-Usage)** — если модель загружена в GPU, в `nvidia-smi` будет занято несколько гигабайт. Скрипт `scripts/check_ollama_gpu.py` по полю `size_vram` из `/api/ps` показывает то же самое: если `size_vram > 0`, модель в VRAM (GPU используется для хранения).
- **GPU-Util (%)** — загрузка «вычислителей» во время генерации. Между запросами или при коротких ответах может быть 0%; во время длинной генерации обычно растёт. Если при **непрерывном** запросе (длинный ответ или стрим) GPU-Util остаётся 0% при ненулевом Memory-Usage — возможна работа части слоёв на CPU (см. `OLLAMA_NUM_GPU` на сервере).
- **Linux (NVIDIA):** во время запроса к модели выполните:
  ```bash
  nvidia-smi
  ```
  Смотрите «Memory-Usage» (должно быть > 0 при загруженной модели) и «GPU-Util» (растёт во время генерации).

- Можно смотреть в реальном времени:
  ```bash
  watch -n 1 nvidia-smi
  ```
  и в другом окне отправить запрос (например `curl` к `/api/generate` или через наш чат).

### 2.2 Что смотрит Ollama при старте

- **Логи сервера Ollama** показывают, какой бэкенд выбран (CUDA/CPU и т.д.):
  - **systemd:** `journalctl -u ollama -n 100 --no-pager`
  - **Docker:** `docker logs <контейнер_ollama>`
  - **Вручную:** смотреть вывод `ollama serve` в консоли.

В логах ищите строки про загрузку CUDA/GPU — если там только CPU, значит Ollama на этом сервере решил не использовать видеокарту.

### 2.3 Переменные окружения, которые могут «отключить» GPU

Проверьте, не заданы ли на сервере (в systemd, Docker или в оболочке перед `ollama serve`):

| Переменная | Эффект |
|------------|--------|
| `OLLAMA_NUM_GPU=0` | Явное отключение GPU, только CPU. |
| `CUDA_VISIBLE_DEVICES=-1` или пусто при одной видеокарте | NVIDIA GPU не видна процессу — Ollama будет только на CPU. |

Если нужно **включить** GPU: не задавать `OLLAMA_NUM_GPU=0`, для одной карты не задавать `CUDA_VISIBLE_DEVICES` (или задать, например, `CUDA_VISIBLE_DEVICES=0`).

### 2.4 Требования к драйверу и CUDA

- Драйвер NVIDIA **531+** и GPU с compute capability **5.0+** (RTX 3090 подходит).
- На сервере должен быть установлен корректный драйвер; проверка: `nvidia-smi` выводит карту и версию драйвера.
- Официальный пакет Ollama для Linux обычно уже с поддержкой CUDA; если ставили из другого источника — возможна сборка «только CPU».

---

## 3. Наша сторона: что мы не делаем

- Мы **не** передаём в API Ollama параметры вроде `num_gpu` или опций, принудительно включающих/выключающих GPU.
- В запросах к Ollama уходят только стандартные вещи: модель, сообщения, опции вроде `temperature`, `num_ctx`, `num_predict` (из конфига). Решение «GPU или CPU» целиком на стороне Ollama.

Итого: если GPU на удалённом сервере не нагружается — смотрите логи и окружение **на сервере Ollama**, драйвер и `nvidia-smi`; наш код и конфиг можно не менять для «включения» GPU.
