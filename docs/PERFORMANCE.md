# Производительность и узкие места

## Как замерить

- **Бенчмарк (Ollama + наш API):**  
  `python3 scripts/benchmark_performance.py`  
  Замеряет: прямой Ollama (GET /api/tags, POST /api/chat), при запущенном бэкенде — GET /health, GET /models, POST /chat. Показывает задержки в мс и наценку нашего API на chat.

- **Тесты:**  
  - Быстрые (без реального LLM): `pytest -m "not slow"`  
  - Все, включая долгие интеграционные: `pytest tests/` (таймаут 5+ мин для test_complex_tasks и т.п.)

---

## Узкие места (по коду и потоку данных)

### 1. Первый запрос в чат: list_models до generate

**Где:** `ChatUseCase.execute` / `execute_stream` → `ModelSelector.select_model()` → `_get_available()` → `fetch_models_with_capability()`.

**Что происходит:** Если пользователь не выбрал модель в UI, перед вызовом LLM делается **GET /api/tags** (список моделей с Ollama) для выбора модели по сложности. Кэш на **60 секунд** (`ModelSelector.CACHE_TTL`).

**Влияние:** Первое сообщение в сессии (или после 60 с без запросов) даёт лишний round-trip к удалённому Ollama (~десятки–сотни мс). Дальше — только generate.

**Варианты смягчения:** прогрев кэша при старте приложения (вызвать `select_model` или `list_models` в lifespan); увеличить `CACHE_TTL`; при выбранной в UI модели не вызывать `select_model` (уже не делаем лишний запрос).

### 2. Таймауты Ollama (connect / read)

**Где:** `src/infrastructure/llm/ollama.py` — при создании `AsyncClient` передаётся `httpx.Timeout(connect=15, read=config.timeout, ...)`.

**Что происходит:** При недоступном хосте connect обрывается через 15 с (без бесконечного зависания). Долгая генерация ограничена `config.ollama.timeout` (по умолчанию 300 с в default.toml).

**Влияние:** Корректные таймауты, но при медленной сети или тяжёлой модели возможны ReadTimeout — тогда увеличить `timeout` в конфиге.

### 3. Workflow / Improve: несколько вызовов LLM подряд

**Где:** Workflow (planner → researcher → tests → coder), Improve (plan → code → validate, с retry).

**Что происходит:** Один запрос пользователя порождает **несколько** последовательных вызовов LLM к удалённому Ollama. Каждый — полный round-trip (сеть + инференс).

**Влияние:** Основная задержка — не наш код, а сумма времени инференса на сервере и сетевых задержек. Ускорение: быстрее модель/железо, ближе сервер, стриминг (уже есть для workflow и improve где применимо).

### 4. Intent detection и команды перед LLM

**Где:** `ChatUseCase`: `IntentDetector.detect()` (синхронно), затем разбор команд (`@web`, `@rag`, и т.д.), затем `_build_messages()`.

**Что происходит:** Перед каждым запросом к LLM — детекция на шаблоны (приветствие, помощь) и обработка команд. Если есть команды — возможны вызовы RAG, web search и т.д.

**Влияние:** Обычно доли миллисекунд (regex/парсинг). Заметная задержка только при реальных вызовах RAG или web search — тогда узкое место там (RAG/сеть).

### 5. Память чата (история)

**Где:** `ConversationMemory`: load/save по `conversation_id`; в запросе передаётся история (`request.history` или подгрузка из памяти).

**Что происходит:** При каждом ответе — сохранение истории; при запросе с `conversation_id` — загрузка последних N сообщений. Реализация — файловая (или иная persistence).

**Влияние:** Обычно мало по сравнению с LLM. При очень длинной истории и большом `max_context_messages` рост размера контекста может замедлять сам LLM.

### 6. Стриминг

**Где:** Чат: `execute_stream` → `generate_stream` → Ollama API с `stream=True`. Workflow/Improve: стриминг событий (SSE) уже используется.

**Что происходит:** Ответ от LLama идёт чанками; первый чанк может прийти быстрее, чем полный ответ.

**Влияние:** UX улучшается (быстрее первый токен). Узкое место — по-прежнему задержка до первого токена на стороне Ollama и сеть.

---

## Рекомендации

1. **Замеры:** Регулярно запускать `scripts/benchmark_performance.py` при смене хоста/модели/сети — смотреть на наценку API и абсолютные задержки.
2. **Первый запрос:** При желании уменьшить задержку первого сообщения — прогрев кэша моделей при старте или увеличение `CACHE_TTL`.
3. **Тяжёлые сценарии:** Workflow и Improve упираются в количество и длительность вызовов LLM; оптимизация — выбор более быстрых моделей, стриминг, при необходимости — кэш типовых ответов (если появится такой сценарий).
4. **Тесты:** Для CI держать быстрый набор: `pytest -m "not slow"`. Долгие тесты с реальным LLM запускать отдельно или по тегу.
