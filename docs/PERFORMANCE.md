# Производительность и узкие места

## Главный вывод: слабое место — удалённый сервер

**Узкое место по задержкам — не наше приложение, а удалённый сервер с Ollama (сеть + инференс).**

**В чём именно «слабость»:** не в том, что сервер плохой или медленный. «Слабое место» здесь значит **звено, которое задаёт большую часть задержки**: пока ответ готовится на удалённом Ollama (сеть туда/обратно + инференс), мы не можем отдать пользователю результат быстрее. То есть ограничение по скорости упирается в это звено — в сеть и в инференс на сервере. Сам сервер может работать отлично (как у вас десктопная Ollama); слабость в том, что **любой** удалённый вызов LLM платит эту цену (RTT + время генерации), и сократить общую задержку можно только там — более быстрая сеть, более быстрая модель или железо на сервере.

По замерам бенчмарка (`scripts/benchmark_performance.py`):

| Что замеряем | Типично (мс) | Где время уходит |
|--------------|--------------|------------------|
| **Ollama GET /api/tags** | ~30–50 | Сеть до сервера + ответ Ollama (список моделей). |
| **Ollama POST /api/chat** | ~400–600+ | **Почти всё — удалённый сервер:** инференс модели + сеть туда/обратно. Наш код в этой цифре не участвует. |
| **Наш API GET /health** | ~40–50 | Один запрос к Ollama (`is_available`) + свой код — доли мс. |
| **Наш API GET /models** | ~35–60 | Прокси к Ollama `/api/tags` + сериализация. |
| **Наш API POST /chat** | ~300–500+ | Intent + (при холодном кэше) list_models + **один вызов Ollama chat**. Основная задержка — снова ответ удалённого Ollama. |

**Итог:** Большая часть времени любого запроса с LLM уходит на **сеть до удалённого хоста** и **инференс на удалённом сервере**. Наш слой добавляет десятки миллисекунд (intent, model_selector, память, JSON). Чтобы ускорить отклик — оптимизировать удалённый сервер (модель, железо, сеть), а не наш код.

---

## Насколько это честно и соответствует фактам

**Что мы реально измеряем**

- **«Ollama POST /api/chat»** — один HTTP-запрос с вашей машины (где запущен бенчмарк) **напрямую к удалённому Ollama**. Время = сеть туда/обратно + инференс на сервере. Наш код в этом запросе не участвует.
- **«API POST /chat»** — один HTTP-запрос к нашему бэкенду (localhost); бэкенд внутри делает intent, при необходимости list_models, затем **один вызов того же Ollama** (сеть + инференс). Итого: наша обработка + тот же по сути вызов Ollama.

**Откуда вывод «слабое место — удалённый сервер»**

- Сравниваем два числа: время прямого `POST /api/chat` к Ollama (A) и время нашего `POST /chat` (B).
- Разница (B − A) — это **наценка нашего слоя** (intent, model_selector, память, сериализация). В замерах она обычно в пределах десятков мс (иногда отрицательная из‑за разброса или тёплого кэша).
- Само число A (300–600+ мс) — это **целиком** сеть до удалённого хоста и инференс на нём. Время B того же порядка, потому что внутри один и тот же вызов к Ollama.
- Поэтому корректно говорить: **основная задержка = удалённый сервер (сеть + инференс)**, наш код добавляет относительно малую добавку. Это соответствует фактическому положению при типичной конфигурации (клиент → наш API на localhost → удалённый Ollama).

**Ограничения и оговорки**

- Один замер — один запрос; бывают выбросы и разброс (сеть, загрузка сервера). Имеет смысл гонять бенчмарк несколько раз или при разной нагрузке.
- **Первый** запрос в чат при холодном кэше моделей может включать лишний round-trip (GET /api/tags) — тогда наценка нашего API может быть на десятки мс больше; основная задержка всё равно остаётся вызов Ollama chat.
- Мы не замеряем сценарии с RAG, @web, длинной историей и т.п. — там могут появляться свои узкие места (поиск, веб), но время **ответа LLM** по-прежнему определяется удалённым сервером и сетью.
- Если наш бэкенд стоял бы не на localhost, а за сетью, в «API POST /chat» добавилась бы ещё одна сетевая hop до нашего API; вывод про «слабое место — удалённый Ollama» от этого не меняется.

**Итог:** Формулировка «слабое место по задержкам — удалённый сервер (сеть + инференс)» отражает то, что мы реально измеряем и как устроен поток: один доминирующий вызов к Ollama и небольшая добавка нашего кода. Для вашей схемы (клиент → наш API → удалённый Ollama) это честно и соответствует фактическому положению.

---

## Результаты бенчмарка (пример)

При запуске `scripts/benchmark_performance.py` против удалённого Ollama и локального API (localhost:8000):

- **Ollama напрямую:** GET /api/tags ~30–40 ms, POST /api/chat (короткий ответ) ~450–500 ms — это по сути задержка **удалённого сервера + сети**.
- **Наш API:** наценка нашего слоя обычно десятки мс; полное время POST /chat всё равно определяется временем ответа Ollama.

## Как замерить

- **Бенчмарк (Ollama + наш API):**  
  `python3 scripts/benchmark_performance.py`  
  Замеряет: прямой Ollama (GET /api/tags, POST /api/chat), при запущенном бэкенде — GET /health, GET /models, POST /chat. Показывает задержки в мс и наценку нашего API на chat.

- **Тесты:**  
  - Быстрые (без реального LLM): `pytest -m "not slow"`  
  - Все, включая долгие интеграционные: `pytest tests/` (таймаут 5+ мин для test_complex_tasks и т.п.)

---

## Узкие места (по коду и потоку данных)

### 1. Первый запрос в чат: list_models до generate

**Где:** `ChatUseCase.execute` / `execute_stream` → `ModelSelector.select_model()` → `_get_available()` → `fetch_models_with_capability()`.

**Что происходит:** Если пользователь не выбрал модель в UI, перед вызовом LLM делается **GET /api/tags** (список моделей с Ollama) для выбора модели по сложности. Кэш на **60 секунд** (`ModelSelector.CACHE_TTL`).

**Влияние:** Первое сообщение в сессии (или после 60 с без запросов) даёт лишний round-trip к удалённому Ollama (~десятки–сотни мс). Дальше — только generate.

**Варианты смягчения:** прогрев кэша при старте приложения (вызвать `select_model` или `list_models` в lifespan); увеличить `CACHE_TTL`; при выбранной в UI модели не вызывать `select_model` (уже не делаем лишний запрос).

### 2. Таймауты Ollama (connect / read)

**Где:** `src/infrastructure/llm/ollama.py` — при создании `AsyncClient` передаётся `httpx.Timeout(connect=15, read=config.timeout, ...)`.

**Что происходит:** При недоступном хосте connect обрывается через 15 с (без бесконечного зависания). Долгая генерация ограничена `config.ollama.timeout` (по умолчанию 300 с в default.toml).

**Влияние:** Корректные таймауты, но при медленной сети или тяжёлой модели возможны ReadTimeout — тогда увеличить `timeout` в конфиге.

### 3. Workflow / Improve: несколько вызовов LLM подряд

**Где:** Workflow (planner → researcher → tests → coder), Improve (plan → code → validate, с retry).

**Что происходит:** Один запрос пользователя порождает **несколько** последовательных вызовов LLM к удалённому Ollama. Каждый — полный round-trip (сеть + инференс).

**Влияние:** Основная задержка — не наш код, а сумма времени инференса на сервере и сетевых задержек. Ускорение: быстрее модель/железо, ближе сервер, стриминг (уже есть для workflow и improve где применимо).

### 4. Intent detection и команды перед LLM

**Где:** `ChatUseCase`: `IntentDetector.detect()` (синхронно), затем разбор команд (`@web`, `@rag`, и т.д.), затем `_build_messages()`.

**Что происходит:** Перед каждым запросом к LLM — детекция на шаблоны (приветствие, помощь) и обработка команд. Если есть команды — возможны вызовы RAG, web search и т.д.

**Влияние:** Обычно доли миллисекунд (regex/парсинг). Заметная задержка только при реальных вызовах RAG или web search — тогда узкое место там (RAG/сеть).

### 5. Память чата (история)

**Где:** `ConversationMemory`: load/save по `conversation_id`; в запросе передаётся история (`request.history` или подгрузка из памяти).

**Что происходит:** При каждом ответе — сохранение истории; при запросе с `conversation_id` — загрузка последних N сообщений. Реализация — файловая (или иная persistence).

**Влияние:** Обычно мало по сравнению с LLM. При очень длинной истории и большом `max_context_messages` рост размера контекста может замедлять сам LLM.

### 6. Стриминг

**Где:** Чат: `execute_stream` → `generate_stream` → Ollama API с `stream=True`. Workflow/Improve: стриминг событий (SSE) уже используется.

**Что происходит:** Ответ от LLama идёт чанками; первый чанк может прийти быстрее, чем полный ответ.

**Влияние:** UX улучшается (быстрее первый токен). Узкое место — по-прежнему задержка до первого токена на стороне Ollama и сеть.

---

## Рекомендации

1. **Замеры:** Регулярно запускать `scripts/benchmark_performance.py` при смене хоста/модели/сети — смотреть на наценку API и абсолютные задержки.
2. **Первый запрос:** При желании уменьшить задержку первого сообщения — прогрев кэша моделей при старте или увеличение `CACHE_TTL`.
3. **Тяжёлые сценарии:** Workflow и Improve упираются в количество и длительность вызовов LLM; оптимизация — выбор более быстрых моделей, стриминг, при необходимости — кэш типовых ответов (если появится такой сценарий).
4. **Тесты:** Для CI держать быстрый набор: `pytest -m "not slow"`. Долгие тесты с реальным LLM запускать отдельно или по тегу.
