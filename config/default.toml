# CodeGen AI - Default configuration
# Override with development.toml or environment variables

[server]
host = "0.0.0.0"
port = 8000

[llm]
provider = "ollama"  # "ollama" | "lm_studio"

# Ollama: optional num_ctx (context window), num_predict (max output tokens) for max performance.
# num_ctx: e.g. 32768, 131072 — more VRAM, longer context. Leave unset for model default.
# num_predict: max tokens to generate; -1 = no limit. Leave unset for model default.
[ollama]
host = "http://192.168.178.126:11434"
timeout = 120
pool_size = 4
# num_ctx = 32768
# num_predict = -1

# LM Studio / OpenAI-compatible: optional max_tokens for long generations.
[openai_compatible]
base_url = "http://localhost:1234/v1"
api_key = ""
timeout = 120
# max_tokens = 8192

# Model identifiers by task complexity. Defaults for Ollama.
# Per-provider overrides: [models.lm_studio], [models.ollama], etc.
# LM Studio IDs: "local" for loaded model, or exact ID from GET /v1/models.
# Use models that exist in Ollama; override in Settings or development.toml.
[models]
simple = "qwen2.5-coder:7b"
medium = "qwen2.5-coder:7b"
complex = "qwen2.5-coder:7b"
fallback = "qwen2.5-coder:7b"

# Per-provider overrides. Add [models.<provider>] for any provider.
# LM Studio: "local" = loaded model. Or use different IDs per complexity.
[models.lm_studio]
simple = "local"
medium = "local"
complex = "local"
fallback = "local"

[embeddings]
provider = "auto"
model = "nomic-embed-text"

# rate_limit_requests_per_minute: raise for heavy use (chat + workflow + improve in parallel).
[security]
rate_limit_requests_per_minute = 100
cors_origins = ["http://localhost:5173"]

# max_context_messages: how many past turns to send to the model. Increase for long threads.
[persistence]
output_dir = "output"
max_context_messages = 20

[rag]
chromadb_path = "output/chromadb"
collection_name = "codebase"
chunk_size = 500
chunk_overlap = 50

# Agent: max tool-call iterations per request (Part 4 ROADMAP)
[agent]
max_iterations = 15

# Web search (Cherry Studio–style): SearXNG, Brave, Tavily, Google Custom Search.
# searxng_url: your SearXNG instance (e.g. http://localhost:8080). Leave empty to use public instances.
# brave_api_key: Brave Search API key (2000 free/month). Optional.
# tavily_api_key: Tavily API key from app.tavily.com. Optional.
# google_api_key + google_cx: Google Custom Search (100 free/day). Create at programmablesearchengine.google.com.
# Env overrides: SEARXNG_URL, BRAVE_API_KEY, TAVILY_API_KEY, GOOGLE_API_KEY, GOOGLE_CX
[web_search]
# searxng_url = "http://localhost:8080"
# brave_api_key = ""
# tavily_api_key = ""
# google_api_key = ""
# google_cx = ""

[logging]
level = "INFO"
format = "json"
