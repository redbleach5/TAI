# Руководство по проверке нового проекта

**Цель:** Пошаговый гид для ручной проверки после каждой фазы. Идя по этому документу, вы поймёте, что проверять, зачем и как интерпретировать результат.

**Связано с:** [plan.md](plan.md)

---

## Как пользоваться

1. Завершили фазу → откройте соответствующий раздел.
2. Выполняйте шаги по порядку.
3. Отмечайте чекбоксы по мере проверки.
4. При ошибке — см. «Частые проблемы» в конце раздела.

---

## Фаза 0: Фундамент

### 0.1 Запуск backend

**Зачем:** Убедиться, что приложение стартует без ошибок.

**Как:**
```bash
cd <новый_проект>
source .venv/bin/activate
uvicorn src.main:app --reload --port 8000
```

**Ожидаемо:** В консоли нет traceback; сообщение вида `Uvicorn running on http://127.0.0.1:8000`.

**Частые проблемы:** ImportError — проверьте `pyproject.toml` и установку зависимостей.

- [v] Backend запускается без ошибок

---

### 0.2 Health endpoint

**Зачем:** Health должен показывать статус LLM backend — это основа fail-fast.

**Как:**
```bash
curl http://localhost:8000/health
```

**Ожидаемо:** JSON с полями `llm_provider` ("ollama" или "lm_studio") и `llm_available` (true/false).

**Проверка:**
- Ollama запущен → `llm_available: true`
- Ollama остановлен → `llm_available: false`, без падения backend

**Частые проблемы:** Если при остановленном Ollama backend падает или зависает — кэш не инвалидируется (см. план, раздел 7.1).

- [v] `/health` возвращает `llm_provider` и `llm_available`
- [v] Ollama вкл → `llm_available: true`
- [ ] Ollama выкл → `llm_available: false`, backend работает

---

### 0.3 Смена provider (Ollama ↔ LM Studio)

**Зачем:** Оба backend должны поддерживаться; смена не должна ломать приложение.

**Как:**
1. В `config/default.toml` установите `provider = "ollama"`.
2. Перезапустите backend, проверьте `/health`.
3. Установите `provider = "lm_studio"`.
4. Запустите LM Studio, включите Local Server.
5. Перезапустите backend, проверьте `/health`.

**Ожидаемо:** Health отражает выбранный provider; при lm_studio и запущенном LM Studio — `llm_available: true`.

- [v] Смена provider в config → health отражает backend
- [v] LM Studio (если есть): те же проверки

---

### 0.4 CORS

**Зачем:** Frontend на другом порту (например, 5173) должен подключаться к API.

**Как:**
1. Запустите frontend: `cd frontend && npm run dev`.
2. Откройте http://localhost:5173.
3. В DevTools (Network) проверьте запросы к backend — не должно быть CORS-ошибок.

**Ожидаемо:** Запросы к `http://localhost:8000` проходят; в консоли нет `Access-Control-Allow-Origin`.

- [v] Frontend с другого порта подключается к API без CORS-ошибок

---

### 0.5 Rate limiting

**Зачем:** Защита от DoS; при превышении лимита — 429.

**Как:**
```bash
# Быстро >100 запросов (подстройте под config)
for i in {1..110}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8000/health; done
```

**Ожидаемо:** После превышения лимита (например, 100/мин) — 429 Too Many Requests.

- [v] Превышение rate limit → 429

---

### 0.6 Тесты и линтер

**Зачем:** Базовое качество кода; регрессии при изменениях.

**Как:**
Активировать виртуальное окружение source .venv/bin/activate
pip install -e ".[dev]"
pytest


```bash
pytest
ruff check src/
```

**Ожидаемо:** Все тесты проходят; ruff без ошибок.

- [v] `pytest` проходит
- [v] `ruff check` без ошибок

---

## Фаза 1: Chat режим

### 1.1 Отправка сообщения

**Зачем:** Основной сценарий — пользователь пишет, получает ответ.

**Как:**
1. Откройте UI (http://localhost:5173).
2. Введите: «Объясни, что такое рекурсия».
3. Отправьте.

**Ожидаемо:** Ответ появляется в UI (синхронно или по частям через SSE).

**Частые проблемы:** Пустой ответ — проверьте логи backend; ошибка подключения к LLM — проверьте `/health`.

- [v] Сообщение отправлено → ответ появляется в UI

---

### 1.2 Greeting — быстрый ответ без workflow

**Зачем:** «Привет» не должен запускать полный workflow (planner, coder и т.д.) — это экономит время и ресурсы.

**Как:**
1. Введите: «Привет» или «Hello».
2. Отправьте.
3. Проверьте логи backend — не должно быть вызовов planner, coder, researcher.

**Ожидаемо:** Быстрый ответ (1–2 сек); в логах только intent + chat, без workflow.

- [v] «Привет» → быстрый ответ, без полного workflow

---

### 1.3 SSE stream

**Зачем:** Ответ должен приходить по частям, а не одним блоком — лучше UX.

**Как:**
1. Введите длинный запрос (например, «Напиши функцию сортировки с объяснением»).
2. Наблюдайте за ответом — он должен появляться постепенно.

**Ожидаемо:** Текст «печатается» по частям, а не появляется целиком в конце.

- [phase 3] SSE stream: ответ приходит по частям

---

### 1.4 Сохранение диалогов

**Зачем:** Диалоги должны сохраняться для истории.

**Как:**
1. Отправьте несколько сообщений.
2. Проверьте `output/conversations/` (или путь из config).
3. Должны появиться JSON-файлы с диалогами.

**Ожидаемо:** Файлы создаются; при перезагрузке страницы диалог можно загрузить (если реализовано).

- [v] Диалоги сохраняются в output/conversations/

---

### 1.5 Layout (chat / workflow / ide / split)

**Зачем:** Пользователь должен переключать режимы отображения.

**Как:**
1. Переключите вкладки: Chat, Workflow, IDE, Split.
2. Chat — диалог; Workflow — генерация кода (plan → tests → code); IDE — placeholder; Split — chat + IDE.

**Ожидаемо:** UI меняется; нет ошибок в консоли.

- [v] Layout переключается (chat / workflow / ide / split)

---

### 1.6 Нет дублирования sync/stream

**Зачем:** В текущем проекте было ~70% дублирования — два набора агентов. В новом — один.

**Как:**
```bash
# Проверьте: нет пар planner.py + stream_planner.py
ls -la src/infrastructure/agents/  # или аналог
```

**Ожидаемо:** Один файл на агента (planner, coder и т.д.); нет `stream_planner`, `stream_coder` и т.п.

- [v] Один chat handler, нет дублирования sync/stream агентов

---

## Фаза 2: Model Router

### 2.1 Выбор модели по сложности

**Зачем:** Простые задачи — лёгкая модель (быстро); сложные — тяжёлая (качественно). Сложность определяется эвристикой — одинаково для всех провайдеров.

**Как:**
1. Отправьте простой запрос: «Что такое переменная?» → должна использоваться simple-модель.
2. Отправьте сложный: «Напиши полноценный REST API с аутентификацией» → complex-модель.
3. Проверьте логи или UI (если показывается модель).

**Ожидаемо:** Разные модели для разных задач.

- [v] Простая задача → лёгкая модель
- [v] Сложная задача → тяжёлая модель

---

### 2.2 Per-provider overrides (config-driven)

**Зачем:** LM Studio использует другие ID моделей (например, "local"); Ollama — "qwen2.5-coder:7b". Конфиг должен задавать модели per-provider без хардкода в коде.

**Как:**
1. Проверьте `config/default.toml` — есть `[models]` (defaults) и `[models.lm_studio]` (overrides).
2. `provider = "ollama"` → используются модели из `[models]`.
3. `provider = "lm_studio"` → используются модели из `[models.lm_studio]`.
4. Добавление нового провайдера — только новая секция `[models.<provider>]` в конфиге.

**Ожидаемо:** Нет проверок `if provider == "lm_studio"` в ModelRouter; выбор модели через `config.get_models_for_provider(provider)`.

- [v] Per-provider overrides в config
- [v] ModelRouter без хардкода провайдеров

---

### 2.3 Смена provider → обновление списка моделей

**Зачем:** При переключении Ollama ↔ LM Studio список моделей должен обновляться.

**Как:**
1. `provider = "ollama"` → `GET /models` возвращает модели Ollama.
2. `provider = "lm_studio"` → `GET /models` возвращает модели LM Studio.
3. Перезапустите backend после смены config.

**Ожидаемо:** Список моделей соответствует выбранному provider.

- [v] Смена provider → список моделей обновляется

---

### 2.4 Остановка backend — fail-fast, без зацикливания

**Зачем:** Критическая проверка из текущего проекта — при недоступном Ollama/LM Studio не должно быть цикла попыток с устаревшим кэшем.

**Как:**
1. Остановите Ollama (или LM Studio).
2. Отправьте сообщение в chat.
3. Наблюдайте: должна быть понятная ошибка, а не бесконечные попытки.

**Ожидаемо:** Сообщение об ошибке («LLM недоступен», «Connection refused» и т.п.); нет зацикливания.

- [ ] Ollama/LM Studio выкл → понятная ошибка, без зацикливания

---

### 2.5 Fallback при недоступной модели

**Зачем:** Если выбранная модель недоступна, должна использоваться запасная.

**Как:**
1. В config укажите модель, которой нет (например, `simple = "nonexistent"`).
2. Отправьте простой запрос.
3. Должна использоваться fallback-модель.

**Ожидаемо:** Запрос выполняется с другой моделью; в логах — fallback.

- [ ] Недоступная модель → fallback срабатывает

---

## Фаза 3: Workflow

### 3.1 Полный workflow для code-запроса

**Зачем:** Запрос на генерацию кода должен проходить весь путь: intent → planner → researcher (stub) → tests → coder → validator → END.

**Как:**
1. Откройте вкладку **Workflow** в UI.
2. Введите: «Напиши функцию факториала на Python».
3. Нажмите «Запустить» (с чекбоксом «Стриминг» или без).
4. Наблюдайте: план → тесты → код → валидация.

**Ожидаемо:** Workflow выполняется; в UI видны этапы (plan, tests, code, validation); код появляется. API: `POST /workflow` (sync) или `POST /workflow?stream=true` (SSE).

- [v] «Напиши функцию X» → полный workflow (plan → tests → code → validate)

---

### 3.2 Greeting → END без workflow

**Зачем:** Greeting не должен запускать planner, coder и т.д.

**Как:**
1. Во вкладке Workflow введите: «Привет».
2. Нажмите «Запустить».
3. Проверьте логи — только intent, затем END; нет planner, coder, researcher.

**Ожидаемо:** Быстрый ответ (template); в графе переход intent → END (should_skip_greeting).

- [v] «Привет» → END сразу, без planner/coder

---

### 3.3 Стриминг кода в UI

**Зачем:** Код должен появляться по мере генерации, а не одним блоком в конце.

**Как:**
1. Во вкладке Workflow включите чекбокс «Стриминг».
2. Запрос: «Напиши функцию сортировки пузырьком».
3. Наблюдайте за панелью — план, тесты, код должны «печататься» по частям.

**Ожидаемо:** Инкрементальное появление plan/tests/code в UI.

- [v] Код стримится в UI по мере генерации

---

### 3.4 Нет дублирования агентов

**Зачем:** Один файл на агента — planner, coder и т.д.; не planner + stream_planner.

**Как:**
```bash
ls src/infrastructure/agents/  # или domain/agents/, application/agents/
```

**Ожидаемо:** Один `planner.py`, один `coder.py`; нет `stream_planner.py`, `stream_coder.py`.

- [v] Один файл на агента, нет stream_* дубликатов

---

### 3.5 Retry при таймауте LLM

**Зачем:** Временные ошибки (таймаут, сеть) не должны сразу падать — retry даёт шанс восстановиться.

**Как:**
1. Временно установите очень короткий timeout в config (например, 1 сек).
2. Отправьте запрос, требующий длинной генерации.
3. Проверьте логи — должны быть повторные попытки (2–3).

**Ожидаемо:** В логах retry; в итоге либо успех, либо понятная ошибка после попыток.

- [ ] Таймаут LLM → retry в логах

---

### 3.6 LangGraph checkpointing — resume

**Зачем:** При прерывании (падение backend, обновление страницы) workflow можно возобновить.

**Как:**
1. Запустите длинный workflow (например, «Напиши REST API»).
2. Прервите (Ctrl+C backend или закройте вкладку).
3. Возобновите (если реализовано) — state должен восстановиться.

**Ожидаемо:** State восстанавливается; можно продолжить с последнего checkpoint. (Если resume не реализован в MVP — можно пропустить.)

- [ ] Checkpointing: прервать и возобновить — state восстанавливается (если реализовано)

---

### 3.7 Ошибки workflow отображаются в UI

**Зачем:** При сбое (LLM недоступен, таймаут, ошибка валидации) пользователь должен видеть сообщение, а не «зависший» интерфейс.

**Как:**
1. Остановите LLM (Ollama/LM Studio) или укажите неверную модель.
2. Запустите workflow.
3. В UI должно появиться сообщение об ошибке.

**Ожидаемо:** Ошибка отображается в WorkflowPanel; loading сбрасывается.

- [v] Ошибка workflow → сообщение в UI

---

## Фаза 4: RAG и контекст

### 4.1 Индексация проекта

**Зачем:** RAG должен индексировать кодовую базу для поиска релевантных фрагментов.

**Как:**
1. Запустите Ollama и загрузите модель embeddings: `ollama pull nomic-embed-text`
2. Индексируйте проект: `curl -X POST "http://localhost:8000/rag/index?path=."`
3. Проверьте статус: `curl http://localhost:8000/rag/status`

**Ожидаемо:** Индексация завершается; `documents` > 0 в ответе `/rag/status`.

- [ ] Индексация проекта → статус «готово» (API есть; нужен `ollama pull nomic-embed-text`; индексация может занять 1–2 мин)

---

### 4.2 Контекст из RAG в workflow

**Зачем:** При запросе «добавь функцию в utils.py» в контекст должен попадать код из utils.py.

**Как:**
1. Индексируйте проект с файлом `utils.py`.
2. Запрос: «Добавь функцию validate_email в utils.py».
3. Проверьте логи или ответ — в контексте должен быть код из utils.py.

**Ожидаемо:** Сгенерированный код учитывает существующий код; в логах видны чанки из RAG.

- [v] Запрос «добавь в utils.py» → в контексте есть код из utils.py

---

### 4.3 Embeddings при смене provider

**Зачем:** RAG использует embeddings; при Ollama — Ollama embeddings, при LM Studio — LM Studio.

**Как:**
1. `provider = "ollama"` → индексируйте проект; запрос использует RAG.
2. `provider = "lm_studio"` → то же самое.
3. Оба должны работать.

**Ожидаемо:** RAG работает с обоими provider; нет ошибок «embeddings недоступны».

- [v] Смена provider → embeddings работают

---

### 4.4 Web search (если включён)

**Зачем:** Web search опционален; при недоступности (нет API key, сеть) не должен падать workflow.

**Как:**
1. Включите web search в config.
2. Отправьте запрос, требующий веб-поиска (или отключите сеть).
3. Workflow не должен падать.

**Ожидаемо:** При недоступности — graceful degradation; workflow продолжается без веб-контекста.

- [~] Web search — не реализовано в MVP (опционально Phase 6)

---

## Фаза 5: IDE и артефакты

### 5.1 Код в IDE

**Зачем:** Сгенерированный код должен отображаться в IDE-панели.

**Как:**
1. Запрос на генерацию кода.
2. Переключитесь на layout split или ide.
3. Код должен быть виден в редакторе.

**Ожидаемо:** Код отображается с подсветкой; можно переключать табы при нескольких файлах.

- [v] Сгенерированный код отображается в IDE (Monaco Editor)

---

### 5.2 Copy и Download

**Зачем:** Пользователь должен копировать и скачивать код.

**Как:**
1. Нажмите Copy — код в буфере обмена.
2. Нажмите Download — файл скачивается.

**Ожидаемо:** Copy вставляет код; Download сохраняет файл с правильным именем.

- [v] Copy — код в буфере
- [v] Download — файл скачивается

---

### 5.3 Run (если реализовано)

**Зачем:** Выполнение кода должно быть безопасным (subprocess, таймаут, изоляция).

**Как:**
1. Сгенерируйте простой код (например, `print("hello")`).
2. Нажмите Run.
3. Проверьте вывод.

**Ожидаемо:** Код выполняется; при таймауте или ошибке — понятное сообщение. Если Run не реализован — кнопка «Copy to run locally».

- [v] Run: subprocess с таймаутом через /code/run API

---

### 5.4 Layout split

**Зачем:** Чат и IDE должны быть видны одновременно.

**Как:**
1. Переключите layout на split.
2. Отправьте запрос на генерацию кода (из Chat или Workflow).
3. Чат слева/сверху, IDE справа/снизу — оба видны.

**Ожидаемо:** Оба панели видны; можно скроллить независимо.

- [v] Layout split — чат и IDE видны одновременно

---

## Фаза 6: Reasoning, интеграции, полировка

### 6.1 Reasoning-модель (если есть)

**Зачем:** Модели с `<think>` (DeepSeek-R1, QwQ) должны стримить thinking в UI.

**Как:**
1. Используйте reasoning-модель.
2. Отправьте запрос.
3. В UI должен появляться блок thinking (рассуждения) по мере генерации.

**Ожидаемо:** Thinking стримится; парсинг `<think>` работает.

- [v] `<think>` парсится, thinking стримится в UI

---

### 6.2 Settings UI

**Зачем:** Пользователь должен менять config через UI.

**Как:**
1. Откройте Settings.
2. Измените параметр (например, модель, provider).
3. Сохраните.
4. Проверьте — изменения должны применяться (перезапуск backend).

**Ожидаемо:** Config сохраняется в config/development.toml; изменения применяются после перезапуска.

- [v] Изменение config в UI сохраняется

---

### 6.3 Документация

**Зачем:** README и ARCHITECTURE нужны для онбординга и поддержки.

**Как:**
1. Прочитайте README — есть ли инструкция по запуску?
2. Прочитайте ARCHITECTURE.md — описаны ли слои и потоки данных?
3. Прочитайте CONTRIBUTING.md — есть ли руководство для контрибьюторов?

**Ожидаемо:** README — как установить и запустить; ARCHITECTURE — схема, слои, ключевые компоненты; CONTRIBUTING — как внести вклад.

- [v] README — инструкция по запуску
- [v] ARCHITECTURE.md — описание слоёв и потоков
- [v] CONTRIBUTING.md — руководство по внесению вклада

---

### 6.4 UI/UX полировка

**Зачем:** Интерфейс должен быть удобным и отзывчивым.

**Как:**
1. IDE: проверьте табы Plan/Tests/Code, номера строк, фидбек при копировании.
2. Workflow: индикатор текущего шага (план → тесты → код → валидация).
3. Chat: markdown-рендеринг (код, списки), кнопка «Новый чат», анимация загрузки.
4. Toast-уведомления: при копировании появляется зелёный toast.
5. Responsive: проверьте на мобильном (или сузьте окно браузера).
6. Health: кнопка обновления статуса.

**Ожидаемо:** Все элементы работают; на мобильных layout адаптируется.

- [v] IDE: табы, номера строк, копирование с toast
- [v] Workflow: индикатор шагов
- [v] Chat: markdown, «Новый чат», анимация
- [v] Responsive layout

---

### 6.5 Полный E2E

**Зачем:** Финальная проверка — весь путь от greeting до кода в IDE.

**Как:**
1. Chat: «Привет» → быстрый ответ.
2. Workflow: «Напиши функцию факториала» → полный workflow (plan → tests → code → validation).
3. Copy/Download — код копируется, toast появляется.
4. Переключение вкладок (Chat, Workflow, IDE, Split, Settings) — всё работает.
5. Settings: изменить provider или модель → сохранить → перезапустить backend.

**Ожидаемо:** Весь сценарий выполняется без ошибок.

- [v] E2E: greeting → chat → workflow → код → copy → settings

---

### 6.6 Тестирование

**Зачем:** Убедиться, что код покрыт тестами.

**Как:**
```bash
python -m pytest tests/ --cov=src --cov-report=term
```

**Ожидаемо:** Покрытие 77%+, все тесты проходят.

- [v] pytest проходит (92+ тестов)
- [v] Покрытие 77%+ (domain, infrastructure adapters, application use cases)
- [v] Unit тесты: LLM, embeddings, RAG, reasoning parser, config loader
- [v] Integration тесты: API routes

---

### 7. Self-Improvement (Phase 7)

**Зачем:** Автоматический анализ и улучшение кодовой базы с использованием LLM.

**Компоненты:**

#### 7.1 FileWriter Agent
```bash
# API
POST /files/write      # Запись с backup
GET /files/read        # Чтение файла  
GET /files/backups     # Список backup-ов
POST /files/restore    # Восстановление из backup
```

- [v] FileWriter создаёт backup перед перезаписью
- [v] Security: запись только в project dir или /tmp
- [v] Unit тесты проходят

#### 7.2 Analyzer Agent
```bash
# Анализ проекта
POST /improve/analyze
# Body: { "path": "src", "include_linter": true, "use_llm": false }
```

- [v] Статический анализ (AST, complexity)
- [v] Детектирование issues: complexity, missing docstring, too many params
- [v] Интеграция с ruff linter (если установлен)
- [v] LLM анализ (опционально)
- [v] Unit тесты проходят

#### 7.3 Improvement Workflow
```
analyze → plan → code → validate → [retry?] → write
```

- [v] LangGraph workflow с retry loop (max 3)
- [v] Streaming через SSE
- [v] Validation: syntax check + py_compile

#### 7.4 Task Queue
```bash
POST /improve/queue/add       # Добавить задачу
GET /improve/queue/status     # Статус очереди
POST /improve/queue/start     # Запустить worker
POST /improve/queue/stop      # Остановить worker
```

- [v] Background worker для обработки очереди
- [v] Статус задач: pending → analyzing → planning → coding → validating → completed/failed

#### 7.5 UI Panel
- [v] Вкладка "Улучшение" в Layout
- [v] Статистика проекта (files, lines, functions, complexity)
- [v] Список issues по файлам
- [v] Рекомендации (suggestions)
- [v] Управление очередью задач

**Как проверить:**
```bash
# Backend
curl -X POST http://localhost:8000/improve/analyze \
  -H "Content-Type: application/json" \
  -d '{"path": "src", "include_linter": true}'

# Frontend
# Открыть http://localhost:5173 → Улучшение
```

**Ожидаемо:**
- Анализ возвращает issues и suggestions
- Улучшение файла создаёт backup
- При ошибке валидации — retry до 3 раз

---

## Сводный чеклист

После прохождения всех фаз можно использовать этот список для быстрой проверки:

| Фаза | Ключевые проверки |
|------|-------------------|
| 0 | Backend стартует, /health, CORS, Rate limit, pytest, ruff |
| 1 | Сообщение → ответ, greeting быстрый, SSE, сохранение, layout |
| 2 | Модель по сложности, per-provider overrides, смена provider, fail-fast, fallback |
| 3 | Полный workflow, вкладка Workflow, greeting → END, стриминг кода, ошибки в UI, нет дублирования, retry |
| 4 | Индексация (POST /rag/index), RAG в контексте, embeddings при смене provider |
| 5 | Код в IDE, Copy, Download, Run или «Copy to run», layout split |
| 6 | Reasoning (если есть), Settings UI, README, ARCHITECTURE, E2E |

---

## Частые проблемы

| Проблема | Возможная причина | Что проверить |
|----------|------------------|---------------|
| Backend падает при остановленном Ollama | Устаревший кэш | Инвалидация кэша при недоступности (план 7.1) |
| CORS ошибки | Origins не настроены | config [security] cors_origins |
| Rate limit не срабатывает | Middleware не подключён | slowapi limiter |
| Greeting запускает workflow | Intent не определяет greeting | should_skip_greeting в графе |
| Два набора агентов | Дублирование sync/stream | Один агент с callback (план 7.1) |
| Зацикливание при недоступной модели | Кэш возвращает старый список | Проверка доступности перед возвратом кэша |
| LM Studio: workflow не генерирует код | Неверный ID модели | config [models.lm_studio] simple = "local" и т.д. |
| Workflow зависает без ошибки | Ошибка не пробрасывается в UI | setError при event_type === "error" в useWorkflowStream |
| RAG не возвращает контекст | Индексация не выполнена | API индексации, путь к проекту |
| Embeddings не работают с LM Studio | Provider не переключён | config [embeddings] provider = "auto" |
