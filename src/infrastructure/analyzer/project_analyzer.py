"""Project Analyzer - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—é–±–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.

Production-ready with:
- Pre-compiled regex patterns
- Proper input validation
- Logging for debugging
"""

import logging
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any

from src.infrastructure.analyzer.models import (
    ArchitectureInfo,
    FileMetrics,
    ProjectAnalysis,
    SecurityIssue,
)
from src.infrastructure.analyzer.architecture import analyze_architecture
from src.infrastructure.analyzer.code_smells import find_code_smells
from src.infrastructure.analyzer.file_metrics import compute_file_metrics
from src.infrastructure.analyzer.security_scanner import check_file_security

logger = logging.getLogger(__name__)

# Number of workers for parallel file processing
MAX_WORKERS = 8


class ProjectAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤.
    
    –ü—Ä–æ–≤–æ–¥–∏—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ —Ä–∞–∑–º–µ—Ä
    - –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞ (–º–µ—Ç—Ä–∏–∫–∏)
    - –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å (—É—è–∑–≤–∏–º–æ—Å—Ç–∏)
    - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
    """
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø–æ —è–∑—ã–∫–∞–º
    LANGUAGE_EXTENSIONS = {
        "Python": [".py"],
        "JavaScript": [".js", ".jsx", ".mjs"],
        "TypeScript": [".ts", ".tsx"],
        "HTML": [".html", ".htm"],
        "CSS": [".css", ".scss", ".sass"],
        "JSON": [".json"],
        "YAML": [".yaml", ".yml"],
        "Markdown": [".md", ".mdx"],
        "Shell": [".sh", ".bash"],
        "SQL": [".sql"],
        "TOML": [".toml"],
    }
    
    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
    IGNORE_DIRS = {
        ".git", ".venv", "venv", "node_modules", "__pycache__",
        ".pytest_cache", ".mypy_cache", ".ruff_cache", "dist",
        "build", ".next", "coverage", ".tox", "eggs",
    }

    def __init__(self, max_file_size: int = 1024 * 1024):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞.

        Args:
            max_file_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–±–∞–π—Ç—ã)
        """
        if max_file_size <= 0:
            raise ValueError("max_file_size must be positive")
        self.max_file_size = max_file_size

    def analyze(self, project_path: str) -> ProjectAnalysis:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞.
        
        Args:
            project_path: –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
            
        Returns:
            ProjectAnalysis —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            
        Raises:
            ValueError: If path is invalid or inaccessible
        """
        if not project_path:
            raise ValueError("Project path cannot be empty")
        
        path = Path(project_path).resolve()
        
        if not path.exists():
            raise ValueError(f"Project path does not exist: {project_path}")
        
        if not path.is_dir():
            raise ValueError(f"Project path is not a directory: {project_path}")
        
        # Check read permission
        if not os.access(path, os.R_OK):
            raise ValueError(f"No read permission for: {project_path}")
        
        logger.info(f"Analyzing project: {path}")
        
        analysis = ProjectAnalysis(
            project_path=str(path),
            project_name=path.name,
            analyzed_at=datetime.now().isoformat(),
        )
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã
        files = self._collect_files(path)
        analysis.total_files = len(files)
        
        # File content cache to avoid reading files multiple times
        file_cache: dict[Path, str] = {}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        def analyze_single_file(file_path: Path) -> tuple[FileMetrics | None, list[SecurityIssue], str | None]:
            """Analyze a single file and return results."""
            try:
                metrics = compute_file_metrics(file_path, path)
                security_issues = check_file_security(file_path, path)
                lang = self._detect_language(file_path)
                return metrics, security_issues, lang
            except Exception as e:
                logger.debug(f"Error analyzing {file_path}: {e}")
                return None, [], None
        
        # Use ThreadPoolExecutor for I/O-bound file operations
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_file = {
                executor.submit(analyze_single_file, file_path): file_path
                for file_path in files
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    metrics, security_issues, lang = future.result()
                    
                    if metrics:
                        analysis.file_metrics.append(metrics)
                        analysis.total_lines += metrics.lines_total
                        analysis.total_code_lines += metrics.lines_code
                    
                    if lang:
                        analysis.languages[lang] = analysis.languages.get(lang, 0) + 1
                    
                    analysis.security_issues.extend(security_issues)
                    
                except Exception as e:
                    logger.debug(f"Failed to get result for {file_path}: {e}")
        
        # –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        analysis.architecture = analyze_architecture(path, files)

        # –ü–æ–¥—Å—á—ë—Ç code smells
        analysis.code_smells = find_code_smells(files, path)
        
        # –†–∞—Å—á—ë—Ç scores
        analysis.security_score = self._calculate_security_score(analysis.security_issues)
        analysis.quality_score = self._calculate_quality_score(analysis)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        analysis.recommendations = self._generate_recommendations(analysis)
        analysis.strengths = self._identify_strengths(analysis)
        analysis.weaknesses = self._identify_weaknesses(analysis)
        
        return analysis
    
    def _collect_files(self, path: Path) -> list[Path]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã."""
        files = []
        
        for p in path.rglob("*"):
            if not p.is_file():
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if any(ignored in p.parts for ignored in self.IGNORE_DIRS):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
            try:
                if p.stat().st_size > self.max_file_size:
                    continue
            except OSError:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            if self._detect_language(p):
                files.append(p)
        
        return files
    
    def _detect_language(self, file_path: Path) -> str | None:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é."""
        suffix = file_path.suffix.lower()
        for lang, extensions in self.LANGUAGE_EXTENSIONS.items():
            if suffix in extensions:
                return lang
        return None
    
    def _calculate_security_score(self, issues: list[SecurityIssue]) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç security score."""
        score = 100
        
        # –°—á–∏—Ç–∞–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å –ª–∏–º–∏—Ç–∞–º–∏
        critical_count = sum(1 for i in issues if i.severity == "critical")
        high_count = sum(1 for i in issues if i.severity == "high")
        medium_count = sum(1 for i in issues if i.severity == "medium")
        low_count = sum(1 for i in issues if i.severity == "low")
        
        # Critical/High –≤–ª–∏—è—é—Ç —Å–∏–ª—å–Ω–æ
        score -= min(critical_count * 10, 50)  # Max -50 for criticals
        score -= min(high_count * 5, 25)  # Max -25 for high
        
        # Medium/Low –≤–ª–∏—è—é—Ç —Å–ª–∞–±–æ (—Å –ª–∏–º–∏—Ç–æ–º)
        score -= min(medium_count * 0.5, 15)  # Max -15 for medium
        score -= min(low_count * 0.1, 5)  # Max -5 for low
        
        return max(0, int(score))
    
    def _calculate_quality_score(self, analysis: ProjectAnalysis) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç quality score."""
        score = 70  # –ù–∞—á–∏–Ω–∞–µ–º —Å 70 (neutral)
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ code smells (—Å –ª–∏–º–∏—Ç–æ–º)
        smells_penalty = min(len(analysis.code_smells) * 2, 15)
        score -= smells_penalty
        
        # –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã (–ª–∏–º–∏—Ç)
        large_files = [f for f in analysis.file_metrics if f.lines_code > 500]
        score -= min(len(large_files) * 3, 15)
        
        # –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å (–ª–∏–º–∏—Ç)
        complex_files = [f for f in analysis.file_metrics if f.complexity > 20]
        score -= min(len(complex_files) * 3, 10)
        
        # –ë–æ–Ω—É—Å—ã
        if analysis.total_files > 10:
            score += 10  # Modular structure
        
        if any("test" in f.path.lower() for f in analysis.file_metrics):
            score += 15  # Has tests
        
        if len(analysis.architecture.entry_points) > 0:
            score += 5  # Clear entry points
        
        if len(analysis.languages) > 1:
            score += 5  # Multi-language (full-stack)
        
        # Ratio of comments
        total_comments = sum(f.lines_comment for f in analysis.file_metrics)
        if analysis.total_code_lines > 0:
            comment_ratio = total_comments / analysis.total_code_lines
            if comment_ratio > 0.1:
                score += 5  # Good documentation
        
        return max(0, min(100, score))
    
    def _generate_recommendations(self, analysis: ProjectAnalysis) -> list[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        recs = []
        
        # Security
        critical_issues = [i for i in analysis.security_issues if i.severity == "critical"]
        if critical_issues:
            recs.append(f"üî¥ –ö–†–ò–¢–ò–ß–ù–û: –ò—Å–ø—Ä–∞–≤–∏—Ç—å {len(critical_issues)} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        
        high_issues = [i for i in analysis.security_issues if i.severity == "high"]
        if high_issues:
            recs.append(f"üü† –í–´–°–û–ö–ò–ô: –£—Å—Ç—Ä–∞–Ω–∏—Ç—å {len(high_issues)} –ø—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω–∏")
        
        # Quality
        if len(analysis.code_smells) > 5:
            recs.append(f"‚ôªÔ∏è –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(analysis.code_smells)} code smells")
        
        # Structure
        large_files = [f for f in analysis.file_metrics if f.lines_code > 500]
        if large_files:
            recs.append(f"üì¶ –†–∞–∑–±–∏—Ç—å –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã: {len(large_files)} —Ñ–∞–π–ª–æ–≤ –ø—Ä–µ–≤—ã—à–∞—é—Ç 500 —Å—Ç—Ä–æ–∫")
        
        # Tests
        test_files = [f for f in analysis.file_metrics if "test" in f.path.lower()]
        if not test_files:
            recs.append("üß™ –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã: –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        # Documentation
        doc_files = [f for f in analysis.file_metrics if f.path.endswith(".md")]
        if not doc_files:
            recs.append("üìù –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é: Markdown-—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        return recs[:10]
    
    def _identify_strengths(self, analysis: ProjectAnalysis) -> list[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã."""
        strengths = []
        
        if analysis.security_score >= 80:
            strengths.append("‚úÖ –•–æ—Ä–æ—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        
        if analysis.quality_score >= 70:
            strengths.append("‚úÖ –ü—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞")
        
        if len(analysis.languages) > 1:
            strengths.append(f"‚úÖ –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –ø—Ä–æ–µ–∫—Ç ({', '.join(analysis.languages.keys())})")
        
        if "tests" in str(analysis.architecture.layers.keys()):
            strengths.append("‚úÖ –ï—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤")
        
        if len(analysis.architecture.entry_points) > 0:
            strengths.append("‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Ç–æ—á–∫–∏ –≤—Ö–æ–¥–∞")
        
        avg_complexity = sum(f.complexity for f in analysis.file_metrics) / max(1, len(analysis.file_metrics))
        if avg_complexity < 10:
            strengths.append("‚úÖ –ù–∏–∑–∫–∞—è —Å—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å")
        
        return strengths
    
    def _identify_weaknesses(self, analysis: ProjectAnalysis) -> list[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã."""
        weaknesses = []
        
        if analysis.security_score < 50:
            weaknesses.append("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        elif analysis.security_score < 80:
            weaknesses.append("‚ö†Ô∏è –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")
        
        if analysis.quality_score < 50:
            weaknesses.append("‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞")
        elif analysis.quality_score < 70:
            weaknesses.append("‚ö†Ô∏è –ö–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")
        
        if len(analysis.code_smells) > 10:
            weaknesses.append("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–Ω–æ–≥–æ code smells")
        
        large_files = [f for f in analysis.file_metrics if f.lines_code > 500]
        if large_files:
            weaknesses.append(f"‚ö†Ô∏è {len(large_files)} —Ñ–∞–π–ª–æ–≤ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ")
        
        if not analysis.architecture.entry_points:
            weaknesses.append("‚ö†Ô∏è –ù–µ—Ç —è–≤–Ω—ã—Ö —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞")
        
        return weaknesses
