"""Project Analyzer - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—é–±–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.

Production-ready with:
- Pre-compiled regex patterns
- Proper input validation
- Logging for debugging
"""

import ast
import logging
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)

# Number of workers for parallel file processing
MAX_WORKERS = 8


@dataclass
class FileMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
    path: str
    lines_total: int = 0
    lines_code: int = 0
    lines_comment: int = 0
    lines_blank: int = 0
    functions: int = 0
    classes: int = 0
    complexity: int = 0  # Cyclomatic complexity estimate
    imports: list[str] = field(default_factory=list)
    issues: list[str] = field(default_factory=list)


@dataclass
class SecurityIssue:
    """–ü—Ä–æ–±–ª–µ–º–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
    severity: str  # critical, high, medium, low
    file: str
    line: int
    issue: str
    recommendation: str


@dataclass 
class ArchitectureInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ."""
    layers: dict[str, list[str]] = field(default_factory=dict)
    dependencies: dict[str, list[str]] = field(default_factory=dict)
    entry_points: list[str] = field(default_factory=list)
    config_files: list[str] = field(default_factory=list)


@dataclass
class ProjectAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞."""
    project_path: str
    project_name: str
    analyzed_at: str
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_files: int = 0
    total_lines: int = 0
    total_code_lines: int = 0
    languages: dict[str, int] = field(default_factory=dict)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ñ–∞–π–ª–∞–º
    file_metrics: list[FileMetrics] = field(default_factory=list)
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
    security_issues: list[SecurityIssue] = field(default_factory=list)
    security_score: int = 100  # 0-100
    
    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    architecture: ArchitectureInfo = field(default_factory=ArchitectureInfo)
    
    # –ö–∞—á–µ—Å—Ç–≤–æ
    quality_score: int = 0  # 0-100
    code_smells: list[str] = field(default_factory=list)
    recommendations: list[str] = field(default_factory=list)
    
    # –°–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
    strengths: list[str] = field(default_factory=list)
    weaknesses: list[str] = field(default_factory=list)


class ProjectAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤.
    
    –ü—Ä–æ–≤–æ–¥–∏—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ —Ä–∞–∑–º–µ—Ä
    - –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞ (–º–µ—Ç—Ä–∏–∫–∏)
    - –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å (—É—è–∑–≤–∏–º–æ—Å—Ç–∏)
    - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
    """
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø–æ —è–∑—ã–∫–∞–º
    LANGUAGE_EXTENSIONS = {
        "Python": [".py"],
        "JavaScript": [".js", ".jsx", ".mjs"],
        "TypeScript": [".ts", ".tsx"],
        "HTML": [".html", ".htm"],
        "CSS": [".css", ".scss", ".sass"],
        "JSON": [".json"],
        "YAML": [".yaml", ".yml"],
        "Markdown": [".md", ".mdx"],
        "Shell": [".sh", ".bash"],
        "SQL": [".sql"],
        "TOML": [".toml"],
    }
    
    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
    IGNORE_DIRS = {
        ".git", ".venv", "venv", "node_modules", "__pycache__",
        ".pytest_cache", ".mypy_cache", ".ruff_cache", "dist",
        "build", ".next", "coverage", ".tox", "eggs",
    }
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º word boundaries –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
    SECURITY_PATTERNS = [
        # Critical - —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–π, –Ω–µ —Å—Ç—Ä–æ–∫–∏/–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        (r"(?<!['\"\w])eval\s*\([^)]+\)", "critical", "–í—ã–∑–æ–≤ eval()", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ast.literal_eval() –∏–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã"),
        (r"(?<![\w.])exec\s*\([^)]+\)", "critical", "–í—ã–∑–æ–≤ exec()", "–ü–µ—Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥, –∏–∑–±–µ–≥–∞—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"),
        (r"subprocess\.(call|run|Popen).*shell\s*=\s*True", "critical", "–†–∏—Å–∫ shell injection", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å shell=False –∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–æ–º"),
        (r"os\.system\s*\([^)]+\)", "critical", "–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ OS-–∫–æ–º–∞–Ω–¥", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å subprocess —Å shell=False"),
        
        # High
        (r"pickle\.loads?\s*\(", "high", "–î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è pickle", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å JSON –∏–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"),
        (r"yaml\.load\s*\([^)]*Loader\s*=\s*None", "high", "–ù–µ–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ YAML", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å yaml.safe_load()"),
        (r"(?<!['\"\w])__import__\s*\(", "high", "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–º–ø–æ—Ä—Ç", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏–º–ø–æ—Ä—Ç—ã"),
        (r"password\s*=\s*['\"][a-zA-Z0-9]{8,}['\"]", "high", "–ü–∞—Ä–æ–ª—å –≤ –∫–æ–¥–µ", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ secrets manager"),
        (r"api[_-]?key\s*=\s*['\"][a-zA-Z0-9]{16,}['\"]", "high", "API-–∫–ª—é—á –≤ –∫–æ–¥–µ", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è"),
        
        # Medium - –æ—Ç–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º —à—É–º–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        (r"verify\s*=\s*False", "medium", "–û—Ç–∫–ª—é—á–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ SSL", "–í–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É SSL"),
        
        # Low
        (r"\b(TODO|FIXME|HACK|XXX)\b:", "low", "–ú–∞—Ä–∫–µ—Ä TODO/FIXME", "–ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏"),
    ]
    
    # Code smells
    CODE_SMELL_PATTERNS = [
        (r"def\s+\w+\([^)]{100,}\)", "–î–ª–∏–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (>5)"),
        (r"if\s+.*:\s*\n\s+if\s+.*:\s*\n\s+if", "–ì–ª—É–±–æ–∫–∞—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å (3+ —É—Ä–æ–≤–Ω—è)"),
        (r"except\s*:", "–ü—É—Å—Ç–æ–π except"),
        (r"from\s+\w+\s+import\s+\*", "–ò–º–ø–æ—Ä—Ç —á–µ—Ä–µ–∑ *"),
        (r"global\s+\w+", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ global"),
        (r"#.*type:\s*ignore", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π type: ignore"),
    ]
    
    def __init__(self, max_file_size: int = 1024 * 1024):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞.
        
        Args:
            max_file_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–±–∞–π—Ç—ã)
        """
        if max_file_size <= 0:
            raise ValueError("max_file_size must be positive")
        self.max_file_size = max_file_size
        
        # Pre-compile security patterns for performance
        self._compiled_security_patterns = [
            (re.compile(pattern, re.IGNORECASE), severity, issue, rec)
            for pattern, severity, issue, rec in self.SECURITY_PATTERNS
        ]
        
        # Pre-compile code smell patterns
        self._compiled_smell_patterns = [
            (re.compile(pattern, re.MULTILINE), desc)
            for pattern, desc in self.CODE_SMELL_PATTERNS
        ]
    
    def analyze(self, project_path: str) -> ProjectAnalysis:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞.
        
        Args:
            project_path: –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
            
        Returns:
            ProjectAnalysis —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            
        Raises:
            ValueError: If path is invalid or inaccessible
        """
        if not project_path:
            raise ValueError("Project path cannot be empty")
        
        path = Path(project_path).resolve()
        
        if not path.exists():
            raise ValueError(f"Project path does not exist: {project_path}")
        
        if not path.is_dir():
            raise ValueError(f"Project path is not a directory: {project_path}")
        
        # Check read permission
        if not os.access(path, os.R_OK):
            raise ValueError(f"No read permission for: {project_path}")
        
        logger.info(f"Analyzing project: {path}")
        
        analysis = ProjectAnalysis(
            project_path=str(path),
            project_name=path.name,
            analyzed_at=datetime.now().isoformat(),
        )
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã
        files = self._collect_files(path)
        analysis.total_files = len(files)
        
        # File content cache to avoid reading files multiple times
        file_cache: dict[Path, str] = {}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        def analyze_single_file(file_path: Path) -> tuple[FileMetrics | None, list[SecurityIssue], str | None]:
            """Analyze a single file and return results."""
            try:
                metrics = self._analyze_file(file_path, path)
                security_issues = self._check_security(file_path, path)
                lang = self._detect_language(file_path)
                return metrics, security_issues, lang
            except Exception as e:
                logger.debug(f"Error analyzing {file_path}: {e}")
                return None, [], None
        
        # Use ThreadPoolExecutor for I/O-bound file operations
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_file = {
                executor.submit(analyze_single_file, file_path): file_path
                for file_path in files
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    metrics, security_issues, lang = future.result()
                    
                    if metrics:
                        analysis.file_metrics.append(metrics)
                        analysis.total_lines += metrics.lines_total
                        analysis.total_code_lines += metrics.lines_code
                    
                    if lang:
                        analysis.languages[lang] = analysis.languages.get(lang, 0) + 1
                    
                    analysis.security_issues.extend(security_issues)
                    
                except Exception as e:
                    logger.debug(f"Failed to get result for {file_path}: {e}")
        
        # –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        analysis.architecture = self._analyze_architecture(path, files)
        
        # –ü–æ–¥—Å—á—ë—Ç code smells
        analysis.code_smells = self._find_code_smells(files, path)
        
        # –†–∞—Å—á—ë—Ç scores
        analysis.security_score = self._calculate_security_score(analysis.security_issues)
        analysis.quality_score = self._calculate_quality_score(analysis)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        analysis.recommendations = self._generate_recommendations(analysis)
        analysis.strengths = self._identify_strengths(analysis)
        analysis.weaknesses = self._identify_weaknesses(analysis)
        
        return analysis
    
    def _collect_files(self, path: Path) -> list[Path]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã."""
        files = []
        
        for p in path.rglob("*"):
            if not p.is_file():
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if any(ignored in p.parts for ignored in self.IGNORE_DIRS):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
            try:
                if p.stat().st_size > self.max_file_size:
                    continue
            except OSError:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            if self._detect_language(p):
                files.append(p)
        
        return files
    
    def _detect_language(self, file_path: Path) -> str | None:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é."""
        suffix = file_path.suffix.lower()
        for lang, extensions in self.LANGUAGE_EXTENSIONS.items():
            if suffix in extensions:
                return lang
        return None
    
    def _analyze_file(self, file_path: Path, base_path: Path) -> FileMetrics:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª."""
        rel_path = str(file_path.relative_to(base_path))
        metrics = FileMetrics(path=rel_path)
        
        try:
            content = file_path.read_text(encoding="utf-8", errors="replace")
        except OSError:
            return metrics
        
        lines = content.split("\n")
        metrics.lines_total = len(lines)
        
        in_multiline_comment = False
        
        for line in lines:
            stripped = line.strip()
            
            if not stripped:
                metrics.lines_blank += 1
            elif stripped.startswith("#") or stripped.startswith("//"):
                metrics.lines_comment += 1
            elif '"""' in stripped or "'''" in stripped:
                metrics.lines_comment += 1
                if stripped.count('"""') == 1 or stripped.count("'''") == 1:
                    in_multiline_comment = not in_multiline_comment
            elif in_multiline_comment:
                metrics.lines_comment += 1
            else:
                metrics.lines_code += 1
        
        # Python-specific –∞–Ω–∞–ª–∏–∑
        if file_path.suffix == ".py":
            try:
                tree = ast.parse(content)
                metrics.functions = sum(1 for node in ast.walk(tree) if isinstance(node, ast.FunctionDef))
                metrics.classes = sum(1 for node in ast.walk(tree) if isinstance(node, ast.ClassDef))
                metrics.imports = self._extract_imports(tree)
                metrics.complexity = self._estimate_complexity(tree)
            except SyntaxError:
                metrics.issues.append("Syntax error in file")
        
        return metrics
    
    def _extract_imports(self, tree: ast.AST) -> list[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–ø–æ—Ä—Ç—ã –∏–∑ AST."""
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)
        return imports
    
    def _estimate_complexity(self, tree: ast.AST) -> int:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å."""
        complexity = 1  # Base complexity
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1
            elif isinstance(node, (ast.And, ast.Or)):
                complexity += 1
        
        return complexity
    
    def _check_security(self, file_path: Path, base_path: Path) -> list[SecurityIssue]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ñ–∞–π–ª –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
        issues = []
        rel_path = str(file_path.relative_to(base_path))
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∏ —Ç–µ—Å—Ç—ã –¥–ª—è security –∞–Ω–∞–ª–∏–∑–∞
        if file_path.suffix in [".md", ".mdx", ".rst", ".txt"]:
            return issues
        # More robust test file exclusion
        path_lower = rel_path.lower()
        if any(part in path_lower for part in ["test", "tests", "spec", "__tests__"]) and file_path.suffix == ".py":
            return issues  # Tests often have intentional "bad" patterns
        
        try:
            content = file_path.read_text(encoding="utf-8", errors="replace")
        except OSError as e:
            logger.debug(f"Failed to read {rel_path}: {e}")
            return issues
        
        lines = content.split("\n")
        
        for i, line in enumerate(lines, 1):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            stripped = line.strip()
            if stripped.startswith("#") or stripped.startswith("//"):
                continue
            if stripped.startswith('"""') or stripped.startswith("'''"):
                continue
            
            # Use pre-compiled patterns for performance
            for compiled_pattern, severity, issue, recommendation in self._compiled_security_patterns:
                if compiled_pattern.search(line):
                    issues.append(SecurityIssue(
                        severity=severity,
                        file=rel_path,
                        line=i,
                        issue=issue,
                        recommendation=recommendation,
                    ))
        
        return issues
    
    def _find_code_smells(self, files: list[Path], base_path: Path) -> list[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç code smells using pre-compiled patterns."""
        smells = []
        
        for file_path in files:
            if file_path.suffix != ".py":
                continue
            
            try:
                content = file_path.read_text(encoding="utf-8", errors="replace")
            except OSError:
                continue
            
            rel_path = str(file_path.relative_to(base_path))
            
            # Use pre-compiled patterns
            for compiled_pattern, description in self._compiled_smell_patterns:
                matches = compiled_pattern.findall(content)
                if matches:
                    smells.append(f"{rel_path}: {description} ({len(matches)} occurrences)")
        
        return smells[:20]  # Limit to 20
    
    def _analyze_architecture(self, path: Path, files: list[Path]) -> ArchitectureInfo:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞."""
        arch = ArchitectureInfo()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∏ –ø–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
        for file_path in files:
            rel = file_path.relative_to(path)
            parts = rel.parts
            
            if len(parts) > 1:
                layer = parts[0]
                if layer not in arch.layers:
                    arch.layers[layer] = []
                arch.layers[layer].append(str(rel))
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫–∏ –≤—Ö–æ–¥–∞
        entry_patterns = ["main.py", "app.py", "run.py", "index.py", "__main__.py", "cli.py"]
        for file_path in files:
            if file_path.name in entry_patterns:
                arch.entry_points.append(str(file_path.relative_to(path)))
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        config_patterns = ["*.toml", "*.yaml", "*.yml", "*.json", "*.ini", "*.env*"]
        for file_path in files:
            for pattern in config_patterns:
                if file_path.match(pattern):
                    arch.config_files.append(str(file_path.relative_to(path)))
                    break
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏
        for file_path in files:
            if file_path.suffix != ".py":
                continue
            
            try:
                content = file_path.read_text(encoding="utf-8", errors="replace")
                tree = ast.parse(content)
                imports = self._extract_imports(tree)
                
                rel_path = str(file_path.relative_to(path))
                local_imports = [i for i in imports if not i.startswith(("os", "sys", "re", "json", "typing", "dataclass"))]
                if local_imports:
                    arch.dependencies[rel_path] = local_imports[:10]
            except (SyntaxError, OSError):
                continue
        
        return arch
    
    def _calculate_security_score(self, issues: list[SecurityIssue]) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç security score."""
        score = 100
        
        # –°—á–∏—Ç–∞–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å –ª–∏–º–∏—Ç–∞–º–∏
        critical_count = sum(1 for i in issues if i.severity == "critical")
        high_count = sum(1 for i in issues if i.severity == "high")
        medium_count = sum(1 for i in issues if i.severity == "medium")
        low_count = sum(1 for i in issues if i.severity == "low")
        
        # Critical/High –≤–ª–∏—è—é—Ç —Å–∏–ª—å–Ω–æ
        score -= min(critical_count * 10, 50)  # Max -50 for criticals
        score -= min(high_count * 5, 25)  # Max -25 for high
        
        # Medium/Low –≤–ª–∏—è—é—Ç —Å–ª–∞–±–æ (—Å –ª–∏–º–∏—Ç–æ–º)
        score -= min(medium_count * 0.5, 15)  # Max -15 for medium
        score -= min(low_count * 0.1, 5)  # Max -5 for low
        
        return max(0, int(score))
    
    def _calculate_quality_score(self, analysis: ProjectAnalysis) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç quality score."""
        score = 70  # –ù–∞—á–∏–Ω–∞–µ–º —Å 70 (neutral)
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ code smells (—Å –ª–∏–º–∏—Ç–æ–º)
        smells_penalty = min(len(analysis.code_smells) * 2, 15)
        score -= smells_penalty
        
        # –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã (–ª–∏–º–∏—Ç)
        large_files = [f for f in analysis.file_metrics if f.lines_code > 500]
        score -= min(len(large_files) * 3, 15)
        
        # –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å (–ª–∏–º–∏—Ç)
        complex_files = [f for f in analysis.file_metrics if f.complexity > 20]
        score -= min(len(complex_files) * 3, 10)
        
        # –ë–æ–Ω—É—Å—ã
        if analysis.total_files > 10:
            score += 10  # Modular structure
        
        if any("test" in f.path.lower() for f in analysis.file_metrics):
            score += 15  # Has tests
        
        if len(analysis.architecture.entry_points) > 0:
            score += 5  # Clear entry points
        
        if len(analysis.languages) > 1:
            score += 5  # Multi-language (full-stack)
        
        # Ratio of comments
        total_comments = sum(f.lines_comment for f in analysis.file_metrics)
        if analysis.total_code_lines > 0:
            comment_ratio = total_comments / analysis.total_code_lines
            if comment_ratio > 0.1:
                score += 5  # Good documentation
        
        return max(0, min(100, score))
    
    def _generate_recommendations(self, analysis: ProjectAnalysis) -> list[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        recs = []
        
        # Security
        critical_issues = [i for i in analysis.security_issues if i.severity == "critical"]
        if critical_issues:
            recs.append(f"üî¥ –ö–†–ò–¢–ò–ß–ù–û: –ò—Å–ø—Ä–∞–≤–∏—Ç—å {len(critical_issues)} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        
        high_issues = [i for i in analysis.security_issues if i.severity == "high"]
        if high_issues:
            recs.append(f"üü† –í–´–°–û–ö–ò–ô: –£—Å—Ç—Ä–∞–Ω–∏—Ç—å {len(high_issues)} –ø—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω–∏")
        
        # Quality
        if len(analysis.code_smells) > 5:
            recs.append(f"‚ôªÔ∏è –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(analysis.code_smells)} code smells")
        
        # Structure
        large_files = [f for f in analysis.file_metrics if f.lines_code > 500]
        if large_files:
            recs.append(f"üì¶ –†–∞–∑–±–∏—Ç—å –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã: {len(large_files)} —Ñ–∞–π–ª–æ–≤ –ø—Ä–µ–≤—ã—à–∞—é—Ç 500 —Å—Ç—Ä–æ–∫")
        
        # Tests
        test_files = [f for f in analysis.file_metrics if "test" in f.path.lower()]
        if not test_files:
            recs.append("üß™ –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã: –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        # Documentation
        doc_files = [f for f in analysis.file_metrics if f.path.endswith(".md")]
        if not doc_files:
            recs.append("üìù –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é: Markdown-—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        return recs[:10]
    
    def _identify_strengths(self, analysis: ProjectAnalysis) -> list[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã."""
        strengths = []
        
        if analysis.security_score >= 80:
            strengths.append("‚úÖ –•–æ—Ä–æ—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        
        if analysis.quality_score >= 70:
            strengths.append("‚úÖ –ü—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞")
        
        if len(analysis.languages) > 1:
            strengths.append(f"‚úÖ –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –ø—Ä–æ–µ–∫—Ç ({', '.join(analysis.languages.keys())})")
        
        if "tests" in str(analysis.architecture.layers.keys()):
            strengths.append("‚úÖ –ï—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤")
        
        if len(analysis.architecture.entry_points) > 0:
            strengths.append("‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Ç–æ—á–∫–∏ –≤—Ö–æ–¥–∞")
        
        avg_complexity = sum(f.complexity for f in analysis.file_metrics) / max(1, len(analysis.file_metrics))
        if avg_complexity < 10:
            strengths.append("‚úÖ –ù–∏–∑–∫–∞—è —Å—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å")
        
        return strengths
    
    def _identify_weaknesses(self, analysis: ProjectAnalysis) -> list[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã."""
        weaknesses = []
        
        if analysis.security_score < 50:
            weaknesses.append("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        elif analysis.security_score < 80:
            weaknesses.append("‚ö†Ô∏è –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")
        
        if analysis.quality_score < 50:
            weaknesses.append("‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞")
        elif analysis.quality_score < 70:
            weaknesses.append("‚ö†Ô∏è –ö–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")
        
        if len(analysis.code_smells) > 10:
            weaknesses.append("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–Ω–æ–≥–æ code smells")
        
        large_files = [f for f in analysis.file_metrics if f.lines_code > 500]
        if large_files:
            weaknesses.append(f"‚ö†Ô∏è {len(large_files)} —Ñ–∞–π–ª–æ–≤ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ")
        
        if not analysis.architecture.entry_points:
            weaknesses.append("‚ö†Ô∏è –ù–µ—Ç —è–≤–Ω—ã—Ö —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞")
        
        return weaknesses
